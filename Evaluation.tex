In this chapter, we describe the experiments we conducted to evaluate the performance of the fuzzers we generate.
We measure the performance of the baseline black-box fuzzers, as well as the impact of our two major improvements to them: 
    avoiding generating calls to unavailable subactions and analyzing subaction preconditions to reduce the number of generated illegal arguments.
In addition, we describe another simple technique of automatically generating fuzzers for game descriptions that's completely independent from RL.
We use this technique to generate black-box and white-box fuzzers, and compare their performance with the RLC counterparts to assess the overall performance of using RL
    to fuzz game descriptions.

We evaluate the fuzzers on three sample games.
We introduce bugs in these games for the fuzzers to find.
In addition, we construct the games in a way that allows us to parametrize the minimum number of legal actions to be taken on the game in order to find the bug.
This allows us to investigate how the fuzzers' performance evolve as the bug complexity increases.

\section{Baseline}
In this section, we describe the baseline we use to evaluate the efficiency of fuzzers generated by RLC.
As a baseline, we need a simpler method of automatically generating fuzzers for game descriptions. Any such method
needs to establish an abstraction for how a game is described. We have chosen to use the abstraction of OpenSpiel, 
Google DeepMind's framework for applying reinforcement learning methods to games \cite{OpenSpiel}.

The OpenSpiel repository includes some example games. We describe two simple methods to generate white-box and black-box fuzz targets for these games.
These games, modified minimally to introduce bugs the fuzzer can find, form the baseline of our evaluation.

\subsection{Generating white-box fuzzers for OpenSpiel games} \label{osWhiteBox}
Games in OpenSpiel are described as procedural extensive-form games. Where a game has:
\begin{itemize}
    \item finite set of players. Including a special player representing chance.
    \item A finite set of all possible actions players can take in all game states.
    \item A finite set of histories. Each history is a sequence of actions that were taken from the start
    of the game.
    \item A finite set of terminal histories that represent a finished game.
    \item A utility for each player for each terminal history.
    \item A player assigned to take the next action for each non-terminal history. Including a special player representing simultaneous states, 
        where players act simultaneously choosing a joint action.
    \item A set of states, where each state is a set of histories such that histories in the same state can not be distinguished by the acting player.
\end{itemize}

Complete game descriptions are objects that implement some interface methods exposing the elements 
described above, along with some utility methods to simplify moving from a history to its successors.
In particular, OpenSpiel game descriptions implement the following methods that are useful for generating a fuzzer:
\begin{itemize}
    \item \texttt{Game::NewInitialState}
    \item \texttt{State::isTerminal}
    \item \texttt{State::isChanceNode}
    \item \texttt{State::isSimultaneousNode}
    \item \texttt{State::LegalChanceOutcomes}
    \item \texttt{State::LegalActions}
    \item \texttt{State::ApplyAction}
\end{itemize}
Depending only on these functions, we can generate a fuzz target analogous to the ones we generate for RL descriptions 
as shown in Algorithm~\ref{alg:var}, where $pickOne$ is a function that picks an element of the given set consuming the next $log_2(n)$ bits of 
the fuzz input. After generating the fuzz target, we plug the fuzz target into LLVM's libFuzzer
to generate the complete fuzzer.

\begin{algorithm}[H]
    \caption{White-box fuzzer for OpenSpiel games}
    \label{alg:var}
    \begin{algorithmic}[1]
    \STATE $state \gets game.NewInitialState()$
    \WHILE{$state.isTerminal()$}
        \IF{$state.isChanceNode()$}
            \STATE $nextAction \gets \pmb{pickOne}(state.LegalChanceOutcomes())$
        \ELSIF{$state.isSimultaneousNode()$}
            \STATE $actions \gets []$
            \FOR {$player \in players$}
                \STATE $actions.append(\pmb{pickOne}(state.LegalActions(player)))$
            \ENDFOR
            \STATE $nextAction \gets ApplyAction(actions)$
        \ELSE
            \STATE $nextAction \gets \pmb{pickOne}(state.LegalActions())$
        \ENDIF
        \STATE $state.ApplyAction(nextAction)$
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}

It should be noted that OpenSpiel games may have two kinds of chance nodes. Explicit stochastic chance nodes
expose multiple legal actions, as well as a probability distribution over those actions. On the other hand, 
sampled stochastic chance nodes expose a single action with non-deterministic behavior. This fuzz target is 
only suitable for games with no sampled stochastic chance nodes since the fuzz target has to be deterministic
with respect to the fuzz input.

\subsection{Generating black-box fuzzers for OpenSpiel games} \label{osBlackBox}
The method to generate black-box fuzzers is very similar to its white-box alternative.
The white-box fuzzers have access to perfect information about what actions can be taken on the game at any given state.
They also know the complete action-space of the game.
In contrast, the black-box fuzzers should be analogous to the simple black-box fuzzers we described in Section \ref{blackboxFuzzTargets}.
The white-box fuzzers always pick legal actions thanks to the function call \texttt{state.LegalActions(player)}.
This returns the complete list of legal actions for the current game state, and the white-box fuzzer can pick among them.

Fortunately, OpenSpiel actions are represented by integers.
Therefore, the black-box fuzzers for OpenSpiel games can simply pick any integer as the action to be taken, as opposed to picking from a set of valid integers.
In this way, we obtain fuzzers analogous to the black-box fuzzers generated by RLC.

\section{Benchmarks}
In this section, we describe how our benchmarks are constructed.
We use three games from OpenSpiel samples as our benchmarks: \texttt{blackjack}, \texttt{tic\_tac\_toe} and \texttt{crazy\_eights}.
We modify these games lightly to introduce bugs in them for the fuzzers to find.
In parallel, we implement the same three games in RL, introducing the same bugs.

Furthermore, we want to be able to observe how the fuzzer performances change as the bugs become harder to discover.
In order to achieve this, we modify each game that allows us to control the minimum number of successive legal actions the fuzzer has to take before finding the bug
via a numeric parameter.
We call this parameter bug depth. Each game implements bug depth in a suitable form.

\subsubsection{Blackjack}
Blackjack is a simple card game played with a standard 52 card deck.
Cards 2-9 are worth points equal to their rank, face cards are all worth 10 points and aces are worth the player's choice of 1 or 11 points.
The goal is to get as close to 21 points without going above it.
Players all start the game with 2 cards in their hand.
Players take their turns in sequence. On a player's turn, they can choose to draw any number of cards as long as they do not surpass 21 points.
When they draw a card that puts them above 21 points, they lose the game.

The benchmark is blackjack played with one player and a dealer. The dealer does not explicitly take actions on the game.
When the player passes their turn, the dealer keeps drawing cards as long as they are more than 4 points short of the target score.
The bug is triggered when either the player or the dealer reached the target score exactly.

We modify the game with a bug depth parameter. This parameter controls the number of suits in the deck, as well as the target score.
The deck has $52 * bug\ depth$ cards and the target score is $21 * bug\ depth$. The dealer stops drawing at $21 * bug\ depth - 4$ points.
Since the point values of cards do not scale with bug depth, the number of cards a player has to draw before reaching the target score scales linearly with bug depth. 

\subsubsection{Tic Tac Toe}
Tic tac toe is a game where players take turns marking unmarked spaces in a 3x3 grid.
The first player to mark three spaces in a straight line, including the two diagonals, wins.

The benchmark is a sequence of tic tac toe games.
The two players player as many tic tac toe games as bug depth, and the bug triggers when a player wins the last game by marking all three spaces of the secondary diagonal.
Since the players need to reach the last game for the bug to trigger, they need to take a number of actions that scales linearly with bug depth.

\subsubsection{Crazy Eights}
Crazy eights is a precursor of UNO. It is played with a standard 52 card deck.
At the start fo the game, each player is dealt 7 cards.
Then, the top card of the deck is revealed. On their turn, the player can draw up to three cards, and play up to one card.
They can only play cards that have a matching suit or a matching rank with the last played card, or an eight.
When a player plays an eight, they pick the suit the next player has to match.
A player needs to draw all three cards if they do not play a card. Once a player plays a card, they can not draw more cards than turn.

Crazy eights can include special cards that make players draw additional cards, skip their turn, or reverse the turn order.
Our benchmark does not implement these special cards. The bug triggers when a player tries to draw from an empty deck.
Similarly to blackjack, the bug depth controls how many cards there are in the deck.
An increasing bug depth linearly increases the number of actions to be taken before reaching the bug.

\subsection{Experiments}
We measure the performance of fuzzers generated with six different methods the three games described above.
\begin{description}
    \item[rlc-full] is the fuzzer described in Chapter \ref{solutionDesign} including all mentioned improvements.
    This fuzzer is generated by invoking RLC with the --fuzzer flag.
    \item[rlc-no-fsm] is another fuzzer generated by RLC. However, this one does not avoid generating calls to unavailable subactions.
    It is generated by calling RLC with --fuzzer-avoid-unavailable-subactions=0 in addition to --fuzzer.
    \item[rlc-no-precons] is the version of \texttt{rlc-full} that does not analyze preconditions to generate reasonable subaction arguments.
    It is generated by passing RLC the --fuzzer-analyze-preconditions=0 flag in addition to --fuzzer.
    \item[rlc-blackbox] is generated by turning off both improvements when calling rlc.
    \item[os-whitebox] is the white-box fuzzer for OpenSpiel games described in Section \ref{osWhiteBox}.
    \item[os-blackbox] is the black-box version of \texttt{os-whitebox} described in Section \ref{osBlackBox}
\end{description}

We generate these six fuzzers for each benchmark for various bug depths.
We measure the average time taken to find the bug, as well as the average number of fuzz inputs tested.
For all experiments, we have to enforce some upper limit on the time we allow the fuzzers to search for the bug.
This is necessary since the true measurement will be impractically large for the less advanced fuzzers.
Which makes them very hard to measure and pointless to present here.
We found 150 seconds to be a suitable time limit to run a single fuzzer instance.

\section{Results}
In this section, we present the results of our experiments and discuss our findings.
\subsection{Blackjack}
\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{total_times_blackjack.png}
    \caption{Average fuzzing times for blackjack.}
    \label{totalTimesBlackjack}
\end{figure}
For blackjack, we measured the average fuzzing times and the average number of tested fuzz inputs for bug depths between 1 and 80.
We observed that \texttt{rlc-blackbox} fails to find the bug in less than 150 seconds starting at bug depth 3.
Similarly, \texttt{os-blackbox} hits the execution time threshold starting at bug depth 5.
We expected the black-box fuzzers to perform worse than their white-box counterparts, and this finding matches our expectation.
We observe a minor difference between the two black-box fuzzers.

Comparing \texttt{rlc-no-precons} with the black-box fuzzers, we observe that avoiding generating calls to unavailable subactions has a noticeable
positive impact on the average fuzzing time.
The RL implementation of blackjack has 6 different subactions, a maximum of 2 of those actions can be available at the same time.
Picking the next action from among the available ones eliminates enough illegal action calls to justify the extra computation in this benchmark.
\texttt{rlc-no-precons} can find the bug without exceeding the time threshold for bug depths under 41.

Similarly, we observe that analyzing the preconditions of the subactions in blackjack increases the performance dramatically.
\texttt{rlc-no-fsm} outperforms \texttt{os-whitebox} for bug depths up to 61.
Our method is able to analyze the preconditions for this game perfectly, the fuzzer never generates illegal arguments to a subaction call.
However, it does pick unavailable subactions to call.
Which explains why \texttt{os-whitebox} is the better fuzzer for higher bug depths.

\texttt{rlc-full} and \texttt{os-whitebox} are the two best fuzzers for higher bug depths.
This matches our expectations, as they are the two fuzzers that make maximal use of the available information.
We observe \texttt{os-whitebox} performing worse than \texttt{rlc-no-fsm} for bug depths smaller than 61.
This can be attributed to the fact that when picking an action, \texttt{os-whitebox} enumerates all possible legal actions, pushes them into a list and then picks an action from that list.
This additional computation does not seem to be amortized for short game traces.
However, as the minimum game trace to find the bug gets longer, we can clearly observe the positive impact of this approach.
On the other hand, \texttt{rlc-full} outperforms either for all bug depths.
This is due to the fact that the method we use in RLC does not enumerate all the possible actions and all the possible legal inputs for them.
Instead, it only determines the minimum and the maximum for the inputs.
Which is an equally descriptive constraint for this game.
Therefore, it picks the action inputs from precisely the same set with only a fraction of the overhead.
The reduction of this overhead results in the superior performance of \texttt{rlc-full}.

\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{num_executions_blackjack.png}
    \caption{Average number of tested fuzz inputs for blackjack.}
    \label{avgInputsBlackjack}
\end{figure}

Inspecting the average number of executions in Figure \ref{totalTimesBlackjack}, we observe that the more the fuzzer knows about blackjack, the smaller number of fuzz inputs it needs to try to discover the bug.
This supports our hypothesis.
We also note that the difference between white-box fuzzers and their black-box counterparts in terms of the average number of tested fuzz inputs is 
much more dramatic than the difference in terms of fuzzing time. 
This can be attributed to the fact that white-box fuzzers have a larger overhead on parsing the given fuzz input into subaction invocations.
Hence, they require more time to process a single fuzz input.
Lastly, we observe that the number of tested fuzz inputs have a decreasing trend for \texttt{rlc-blackbox}, \texttt{os-blackbox} and \texttt{rlc-no-precons}.
This is due to the fact that these three fuzzers exceed the time threshold.
The input counts after that point do not represent the number of inputs to find the bug, they represent the number of inputs tested within the threshold.
Which has a decreasing trend because individual game traces get longer as bug depth increases.
\subsection{Tic Tac Toe}
\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{temp_total_times_tic_tac_toe.png}
    \caption{Average fuzzing times for tic tac toe.}
    \label{totalTimesTicTacToe}
\end{figure}
Figure \ref{totalTimesTicTacToe} shows that the fuzzers \texttt{rlc-no-precons}, \texttt{rlc-blackbox} and \texttt{os-blackbox} can not discover the bug within the time constraint even at bug depth 1.
However, the RLC fuzzers that perform constraint analysis can find the bug for bug depths up to 31.
This illustrates the dramatic impact of analyzing constraints, even if the analysis is limited in scope.

Interestingly, we observe that \texttt{rlc-no-fsm} outperforms \texttt{rlc-full} for this benchmark.
This matches neither our expectation nor the result from the blackjack benchmark.
The reason behind this asymmetry is that the RL implementation of tic tac toe only has a single subaction.
Therefore, \texttt{rlc-no-fsm} is as good at choosing an available action as \texttt{rlc-full}.
\texttt{rlc-no-fsm} performs better because it pick the action with less computation overhead.

On the other hand, \texttt{os-whitebox} clearly performs tremendously better than any RLC fuzzer.
To explain this, we need to inspect the precondition of the single subaction in the RL implementation.

\begin{lstlisting}
act mark(Int x, Int y) {
    x < 3,
    x >= 0,
    y < 3,
    y >= 0,
    board.get(x, y) == 0
}
\end{lstlisting}

The technique we describe at Section \ref{preconditionAnalysis} can handle the first four constraints but it 
 can not handle the last constraint \texttt{board.get(x,y) == 0} since this constraint contains a function call with more than one unbound expression as parameters.
It is able to deduce that both variables should be in the range $[0,2]$, but discards the constraint about not marking already mark spaces.
This causes the RLC fuzzers to generate and discard some number of invalid subaction calls.
In contrast, \texttt{os-whitebox} can understand this last constraint as well and never generates invalid actions.
Which explains why the complete whitebox fuzzer is able to significantly outperform the fuzzers generated by RLC.

\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{num_executions_tic_tac_toe.png}
    \caption{Average number of tested fuzz inputs for tic tac toe.}
    \label{avgInputsTicTacToe}
\end{figure}

Figure \ref{avgInputsTicTacToe} mimics our findings for Blackjack.
\texttt{os-whitebox} is the fuzzer that uses most of the available information, and it can find the bug with the smallest number fuzz inputs.
We observe that the difference in the number of fuzz inputs is on a much greater scale tan the difference in total fuzzing time between the white-box and black-box fuzzer for for this benchmark too.
This is especially true for the comparison between \texttt{rlc-full} and \texttt{rlc-no-fsm}.
For small bug depths, \texttt{rlc-full} can find the input in using less fuzz inputs but still takes more total time to test those inputs.

\subsection{Crazy Eights}
\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{total_times_crazy_eights.png}
    \caption{Average fuzzing times for crazy eights.}
    \label{totalTimesCrazyEights}
\end{figure}

Crazy eights is the most complex of our three benchmarks.
It features ten different actions. In addition, the constraints on three of those actions are too complex to be perfectly analyzed by our method.
In Figure \ref{totalTimesCrazyEights}, we notice some similar patterns to the two other benchmarks.
The black-box fuzzers perform the worst, neither of them can find the bug within the time threshold for input depths over 4.
This is in line with our hypothesis, as well as the two other benchmarks.
Once again, we notice that \texttt{os-whitebox} performs significantly better than any RLC fuzzer at large bug depths.
This result is also expected since our constraint analysis falls short and the RLC fuzzers inevitably generate some invalid subaction calls.

What is noticeably different for this benchmark is that \texttt{rlc-no-precons} actually performs better than \texttt{rlc-no-fsm} here.
We believe this is a result of two factors.
First, since this benchmarks includes some preconditions we can not analyze, the effectiveness of the precondition analysis is reduced.
We can still argue that applying the limited precondition analysis has a notable positive impact on the fuzzing time by comparing \texttt{rlc-blackbox} with \texttt{rlc-no-fsm}, but the gap is not as wide as it is in the other two benchmarks.
Second, the action in this benchmark has more subactions than the previous benchmarks.
Furthermore, only a maximum of two of those subactions are available at any given state.
As a result, if the fuzzer does not make an effort to avoid generating calls to unavailable subactions, it generates a much larger percentage of such calls in comparison to the previous benchmarks.
This manifests in an increased benefit for avoiding generating such calls.
The increased benefit of this optimization, combined with the decreased effectiveness of constraint analysis enables \texttt{rlc-no-precons} to outperform \texttt{rlc-no-fsm}.

\begin{figure}[h]
    \centering
    \includegraphics*[width=12cm]{num_executions_crazy_eights.png}
    \caption{Average number of tested fuzz inputs for crazy eights.}
    \label{avgInputsCrazyEights}
\end{figure}

Inspecting Figure \ref{avgInputsCrazyEights}, we observe white-box models needing to test less fuzz inputs before discovering the bug once again.
The exception to this is \texttt{os-blackbox}, which tests less inputs for bug depths above 5.
This is due to the fact that \texttt{os-blackbox} stops execution when it reaches the time limit, not when it discovers the bug.

Comparing the average number of tested fuzz inputs for crazy eights with the other two benchmarks, we notice that the results for \texttt{rlc-no-fsm} and \texttt{rlc-no-precons} are much more similar than they are for the other benchmarks.
We believe this is another result of the two factors that allow \texttt{rlc-no-precons} to find the bug in less time.

In summary, across all three benchmarks, we have observed the white-box fuzzers to perform much better than their black-box counterparts.
The effectiveness of the two improvements we make to RLC fuzzers, avoiding unavailable subactions and analyzing preconditions, vary depending on the benchmark.
Nevertheless, these improvements are demonstrated to increase the fuzzer's performance unless they produce trivial results, such as avoiding unavailable subactions when the action has a single subaction that is always available.

We have not observed a significant difference between the performance of OpenSpiel fuzzers and RLC fuzzers in general.
\texttt{os-whitebox} has been the best performing fuzzer in general, but this can be attributed to the fact that it can avoid illegal action invocations in all three benchmarks, since it has access to a perfect set of legal actions and arguments.
In fact, we have demonstrated that the RLC fuzzers can perform better than the white-box OpenSpiel fuzzer when it can analyze the preconditions equally well in the blackjack benchmark.
